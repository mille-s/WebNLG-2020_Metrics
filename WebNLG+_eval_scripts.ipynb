{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMO1OV6a67TwnjI2wPpXfsg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mille-s/WebNLG-2020_Metrics/blob/main/WebNLG%2B_eval_scripts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1 - Download resources and clone repos\n",
        "# RUN THIS CELL FIRST; when done, drag the predicted files in the \"hypotheses\" folder\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Download WebNLG+ code\n",
        "! git clone https://github.com/WebNLG/GenerationEval.git\n",
        "\n",
        "# Download BLEURT\n",
        "! git clone https://github.com/google-research/bleurt.git\n",
        "! wget https://storage.googleapis.com/bleurt-oss/bleurt-base-128.zip\n",
        "! unzip bleurt-base-128.zip\n",
        "! rm bleurt-base-128.zip\n",
        "! pip3 install --upgrade pip\n",
        "! pip3 install /content/bleurt/. --user\n",
        "! mv bleurt /content/GenerationEval/metrics\n",
        "! mv bleurt-base-128 /content/GenerationEval/metrics/bleurt\n",
        "\n",
        "# Download METEOR\n",
        "! wget https://www.cs.cmu.edu/~alavie/METEOR/download/meteor-1.5.tar.gz\n",
        "! tar -xvf meteor-1.5.tar.gz\n",
        "! mv meteor-1.5 /content/GenerationEval/metrics\n",
        "! rm meteor-1.5.tar.gz\n",
        "\n",
        "# Download repos for creating references\n",
        "! git clone https://gitlab.com/webnlg/corpus-reader.git\n",
        "! git clone https://gitlab.com/shimorina/webnlg-dataset.git\n",
        "import os\n",
        "import os.path\n",
        "hypotheses = '/content/hypotheses'\n",
        "if not os.path.exists(hypotheses):\n",
        "  os.makedirs(hypotheses)\n",
        "\n",
        "# Human eval results and more data\n",
        "#! git clone https://github.com/WebNLG/challenge-2020.git\n",
        "\n",
        "# Install Python dependencies\n",
        "! pip3 install nltk==3.5\n",
        "! pip3 install pyter3==0.3\n",
        "! pip3 install razdel==0.5.0\n",
        "# ! pip3 install tabulate==0.8.7\n",
        "! pip3 install tabulate==0.9\n",
        "# ! pip3 install bert-score==0.3.5\n",
        "! pip3 install bert-score==0.3.13\n",
        "! pip install transformers==3.0.1\n",
        "\n",
        "# Download nltk 'punkt'\n",
        "import sys\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "NMOtghjMovnO",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTANT: Now drag and drop the predicted files in the \"hypotheses\" folder.\n",
        "\n",
        "> Click on the folder icon on the left if you don't see the folders.\n",
        "\n"
      ],
      "metadata": {
        "id": "kQ3ZVNMH7b6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2 - Adapt original codes to Colab\n",
        "# RUN THIS CELL SECOND\n",
        "# Adapt evaluation code to Colab; creates an eval_edited.py file which will be run in the final cell\n",
        "# Adapt reference creation code to Colab; creates a benchmark_reader-edited.py and a generate_references-edited.py files which will be run in the next cell\n",
        "\n",
        "import codecs\n",
        "import re\n",
        "\n",
        "def edit_file(code_lines, landmark, fo, new_code):\n",
        "  x = 0\n",
        "  stop = 'no'\n",
        "  for line in code_lines:\n",
        "    if stop == 'no':\n",
        "      if re.search('^from benchmark_reader import Benchmark', line):\n",
        "        fo.write('from benchmark_reader_edited import Benchmark\\n')\n",
        "      elif re.search('^from benchmark_reader import select_files', line):\n",
        "        fo.write('from benchmark_reader_edited import select_files\\n')\n",
        "      # I don't know why it was set like this; by changing it we get all lexicalisations in ru instead of just 3\n",
        "      elif re.search('^            for lex in entry\\.lexs\\[1::2\\]:  # take every second lexicalisation, i\\.e\\. only ru\\n', line):\n",
        "        fo.write('            for lex in entry.lexs:\\n')\n",
        "      # elif re.search(\"^        with open\\(f'reference\", line):\n",
        "      #   fo.write(\"        with open(f'reference{str(j)}', 'w+') as f:\\n\")\n",
        "      elif re.search(landmark, line):\n",
        "        stop = 'yes'\n",
        "        fo.write(new_code)\n",
        "      else:\n",
        "        fo.write(line)\n",
        "\n",
        "# evaluation code\n",
        "path_eval = '/content/GenerationEval/eval.py'\n",
        "eval_code_lines = codecs.open(path_eval, 'r', 'utf-8').readlines()\n",
        "fo = codecs.open('/content/GenerationEval/eval_edited.py', 'w', 'utf-8')\n",
        "\n",
        "for line in eval_code_lines:\n",
        "  # comment sys.argv bc used later in a different way\n",
        "  if re.search('^sys\\.argv = ', line):\n",
        "    fo.write('#sys.argv = sys.argv[:1]\\n')\n",
        "  # adapt paths to Colab repo\n",
        "  elif re.search('^BLEU_PATH = ', line):\n",
        "    fo.write(\"BLEU_PATH = '/content/GenerationEval/metrics/multi-bleu-detok.perl'\\n\")\n",
        "  elif re.search('^METEOR_PATH = ', line):\n",
        "    fo.write(\"METEOR_PATH = '/content/GenerationEval/metrics/meteor-1.5/meteor-1.5.jar'\\n\")\n",
        "  elif re.search('^def bleurt', line):\n",
        "    fo.write('def bleurt(references, hypothesis, num_refs, checkpoint = \"/content/GenerationEval/metrics/bleurt/bleurt-base-128\"):\\n')\n",
        "  # bleurt throws an error if arguments are not flagged (no positional arguments accepted)\n",
        "  elif re.search('^    scores = scorer\\.score\\(refs, cands\\)', line):\n",
        "    fo.write('    scores = scorer.score(references=refs, candidates=cands)\\n')\n",
        "  # argParser needs arguments passed explicitly to run on Colab\n",
        "  elif re.search('^    args = argParser.parse_args\\(\\)', line):\n",
        "    fo.write('    args = argParser.parse_args(sys.argv[1:])\\n')\n",
        "  # add one decimal to the scores to replicate exactly the WebNLG+ results\n",
        "  elif re.search(\"^        values.append\\(round\\(result\\['bleu_nltk'\\], 2\\)\\)\", line):\n",
        "    fo.write(\"        values.append(round(result['bleu_nltk'], 3))\\n\")\n",
        "  elif re.search(\"^        values.append\\(round\\(result\\['meteor'\\], 2\\)\\)\", line):\n",
        "    fo.write(\"        values.append(round(result['meteor'], 3))\\n\")\n",
        "  elif re.search(\"^        values.append\\(round\\(result\\['chrf\\+\\+'\\], 2\\)\\)\", line):\n",
        "    fo.write(\"        values.append(round(result['chrf++'], 3))\\n\")\n",
        "  elif re.search(\"^        values.append\\(round\\(result\\['ter'\\], 2\\)\\)\", line):\n",
        "    fo.write(\"        values.append(round(result['ter'], 3))\\n\")\n",
        "  elif re.search(\"^        values.append\\(round\\(result\\['bert_precision'\\], 2\\)\\)\", line):\n",
        "    fo.write(\"        values.append(round(result['bert_precision'], 3))\\n\")\n",
        "  elif re.search(\"^        values.append\\(round\\(result\\['bert_recall'\\], 2\\)\\)\", line):\n",
        "    fo.write(\"        values.append(round(result['bert_recall'], 3))\\n\")\n",
        "  elif re.search(\"^        values.append\\(round\\(result\\['bert_f1'\\], 2\\)\\)\", line):\n",
        "    fo.write(\"        values.append(round(result['bert_f1'], 3))\\n\")\n",
        "  elif re.search(\"^        values.append\\(round\\(result\\['bleurt'\\], 2\\)\\)\", line):\n",
        "    fo.write(\"        values.append(round(result['bleurt'], 3))\\n\")\n",
        "  else:\n",
        "    fo.write(line)\n",
        "\n",
        "fo.close()\n",
        "\n",
        "# reference text creation code\n",
        "path_reader = '/content/corpus-reader/benchmark_reader.py'\n",
        "path_ref_generator = '/content/corpus-reader/generate_references.py'\n",
        "reader_code_lines = codecs.open(path_reader, 'r', 'utf-8').readlines()\n",
        "ref_generator_code_lines = codecs.open(path_ref_generator, 'r', 'utf-8').readlines()\n",
        "fo2 = codecs.open('/content/corpus-reader/benchmark_reader_edited.py', 'w', 'utf-8')\n",
        "fo3 = codecs.open('/content/corpus-reader/generate_references_edited.py', 'w', 'utf-8')\n",
        "\n",
        "# I needed to modify the function that gathers files to process, as I didn't manage to make it work for test data otherwise\n",
        "new_code_reader = \"def select_files(topdir, category='', size=(1, 8)):\\n    finalfiles = []\\n    if topdir.endswith('dev') or topdir.endswith('train'):\\n        finaldirs = [topdir+'/'+str(item)+'triples' for item in range(size[0], size[1])]\\n        for item in finaldirs:\\n            finalfiles += [(item, filename) for filename in sorted(listdir(item)) if category in filename and '.xml' in filename]\\n    else:\\n        finalfiles += [(topdir, filename) for filename in sorted(listdir(topdir)) if 'generation-test-data-with-refs' in filename and '.xml' in filename]\\n    return finalfiles\"\n",
        "# On this file I only changed the paths\n",
        "new_code_ref_generator = \"path = '/content/webnlg-dataset/release_v3.0/en/test'\\nrun_on_corpus_per_lang(path, 'en')\\n# Russian\\npath = '/content/webnlg-dataset/release_v3.0/ru/test'\\nrun_on_corpus_per_lang(path, 'ru')\"\n",
        "\n",
        "edit_file(reader_code_lines, '^def select_files', fo2, new_code_reader)\n",
        "edit_file(ref_generator_code_lines, \"^path = '\\./challenge2020_train_dev_v2\", fo3, new_code_ref_generator)\n",
        "\n",
        "fo2.close()\n",
        "fo3.close()\n"
      ],
      "metadata": {
        "id": "cFYQwOxGECQt",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3 -  Create reference files\n",
        "# RUN THIS CELL THIRD\n",
        "# Create reference files for automatic evaluation\n",
        "\n",
        "import shutil\n",
        "\n",
        "data_to_process_en = '/content/GenerationEval/data_to_process/en/references'\n",
        "data_to_process_ru = '/content/GenerationEval/data_to_process/ru/references'\n",
        "data_to_process_small = '/content/GenerationEval/data_small/en/references'\n",
        "\n",
        "def postproc_and_move(ref_file_path, dest_folder):\n",
        "  short_filename = ref_file_path.split('-')[0]\n",
        "  out_path = os.path.join(dest_folder, short_filename)\n",
        "  fo = codecs.open(out_path, 'w', 'utf-8')\n",
        "  file_lines = codecs.open(ref_file_path, 'r', 'utf-8').readlines()\n",
        "  x = 0\n",
        "  while x < len(file_lines) - 1:\n",
        "    fo.write(file_lines[x])\n",
        "    x = x + 1\n",
        "  # remove last linebreak of the file (creates problems with some metrics)\n",
        "  last_line = file_lines[x].split('\\n')[0]\n",
        "  fo.write(last_line)\n",
        "  fo.close()\n",
        "\n",
        "if not os.path.exists(data_to_process_en):\n",
        "  os.makedirs(data_to_process_en)\n",
        "if not os.path.exists(data_to_process_ru):\n",
        "  os.makedirs(data_to_process_ru)\n",
        "if not os.path.exists(data_to_process_small):\n",
        "  os.makedirs(data_to_process_small)\n",
        "\n",
        "# Create references for the test data\n",
        "path_create_refs = '/content/corpus-reader/generate_references_edited.py'\n",
        "! python {path_create_refs}\n",
        "\n",
        "# Move references to the folder they'll be used from\n",
        "postproc_and_move('reference0-en.txt', data_to_process_en)\n",
        "postproc_and_move('reference1-en.txt', data_to_process_en)\n",
        "postproc_and_move('reference2-en.txt', data_to_process_en)\n",
        "postproc_and_move('reference3-en.txt', data_to_process_en)\n",
        "postproc_and_move('reference4-en.txt', data_to_process_en)\n",
        "postproc_and_move('reference0-ru.txt', data_to_process_ru)\n",
        "postproc_and_move('reference1-ru.txt', data_to_process_ru)\n",
        "postproc_and_move('reference2-ru.txt', data_to_process_ru)\n",
        "postproc_and_move('reference3-ru.txt', data_to_process_ru)\n",
        "postproc_and_move('reference4-ru.txt', data_to_process_ru)\n",
        "postproc_and_move('reference5-ru.txt', data_to_process_ru)\n",
        "postproc_and_move('reference6-ru.txt', data_to_process_ru)\n",
        "\n",
        "# Cleanup\n",
        "! rm 'reference0-en.txt'\n",
        "! rm 'reference1-en.txt'\n",
        "! rm 'reference2-en.txt'\n",
        "! rm 'reference3-en.txt'\n",
        "! rm 'reference4-en.txt'\n",
        "! rm 'reference0-ru.txt'\n",
        "! rm 'reference1-ru.txt'\n",
        "! rm 'reference2-ru.txt'\n",
        "! rm 'reference3-ru.txt'\n",
        "! rm 'reference4-ru.txt'\n",
        "! rm 'reference5-ru.txt'\n",
        "! rm 'reference6-ru.txt'\n",
        "\n",
        "# Create references with 10 sentences to test the metrics quickly\n",
        "data_ref0 = codecs.open('/content/GenerationEval/data/en/references/reference0', 'r', 'utf-8').readlines()\n",
        "data_ref1 = codecs.open('/content/GenerationEval/data/en/references/reference1', 'r', 'utf-8').readlines()\n",
        "data_ref2 = codecs.open('/content/GenerationEval/data/en/references/reference2', 'r', 'utf-8').readlines()\n",
        "data_ref3 = codecs.open('/content/GenerationEval/data/en/references/reference3', 'r', 'utf-8').readlines()\n",
        "data_hyp = codecs.open('/content/GenerationEval/data/en/hypothesis', 'r', 'utf-8').readlines()\n",
        "fo0 = codecs.open('/content/GenerationEval/data_small/en/references/reference0', 'w', 'utf-8')\n",
        "fo1 = codecs.open('/content/GenerationEval/data_small/en/references/reference1', 'w', 'utf-8')\n",
        "fo2 = codecs.open('/content/GenerationEval/data_small/en/references/reference2', 'w', 'utf-8')\n",
        "fo3 = codecs.open('/content/GenerationEval/data_small/en/references/reference3', 'w', 'utf-8')\n",
        "fo4 = codecs.open('/content/GenerationEval/data_small/en/hypothesis', 'w', 'utf-8')\n",
        "x = 0\n",
        "while x < 9:\n",
        "  fo0.write(data_ref0[x])\n",
        "  fo1.write(data_ref1[x])\n",
        "  fo2.write(data_ref2[x])\n",
        "  fo3.write(data_ref3[x])\n",
        "  fo4.write(data_hyp[x])\n",
        "  x = x +1\n",
        "fo0.write(data_ref0[x].split('\\n')[0])\n",
        "fo1.write(data_ref1[x].split('\\n')[0])\n",
        "fo2.write(data_ref2[x].split('\\n')[0])\n",
        "fo3.write(data_ref3[x].split('\\n')[0])\n",
        "fo4.write(data_hyp[x].split('\\n')[0])\n",
        "fo0.close()\n",
        "fo1.close()\n",
        "fo2.close()\n",
        "fo3.close()\n",
        "fo4.close()"
      ],
      "metadata": {
        "id": "go_sSKZva8IB",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4 - Run evaluation (collect results in log_eval folder)\n",
        "import glob\n",
        "import os\n",
        "lang = \"en\"#@param [\"en\", \"ru\"]\n",
        "#metrics = 'bleu,meteor,ter,chrf++,bert,bleurt'#@param\n",
        "metrics = 'bleu,meteor,chrf++,bert'#@param\n",
        "# small_test = 'yes' if want to test quickly the code with files that have 10 examples.\n",
        "# Expected output (en):\n",
        "#   BLEU    BLEU NLTK    METEOR    chrF++    TER    BERT-SCORE P    BERT-SCORE R    BERT-SCORE F1    BLEURT\n",
        "# ------  -----------  --------  --------  -----  --------------  --------------  ---------------  --------\n",
        "#  73.02        0.726     0.578     0.877  0.194           0.978           0.971            0.974      0.81\n",
        "small_test = 'no'#@param [\"yes\", \"no\"]\n",
        "log_folder = os.path.join('/content', 'log_eval')\n",
        "if not os.path.exists(log_folder):\n",
        "  os.makedirs(log_folder)\n",
        "\n",
        "num_refs = ''\n",
        "if lang == 'en':\n",
        "  num_refs = '4'\n",
        "elif lang == 'ru':\n",
        "  num_refs = '6'\n",
        "\n",
        "# metrics = 'bleu,meteor,ter,chrf++,bert,bleurt'\n",
        "# Approximate times below on English dataset (1779 texts) with no GPU\n",
        "# Very fast: BLEU, CHRF++ (~15 sec)\n",
        "# Fast: METEOR (~40 sec)\n",
        "# Slow: TER (~10 min, can't use GPU)\n",
        "# Very slow (Need GPU): BertScore, Bleurt (~1h+ or fail without GPU)\n",
        "path_code_eval = '/content/GenerationEval/eval_edited.py'\n",
        "path_hyp_uploaded = '/content/hypotheses/'\n",
        "if small_test == 'yes':\n",
        "  path_ref_eval = '/content/GenerationEval/data_small/en/references/reference'\n",
        "  path_hyp_eval = '/content/GenerationEval/data_small/en/hypothesis'\n",
        "  log_small = os.path.join(log_folder, 'log_eval_small.txt')\n",
        "  ! python {path_code_eval} -R {path_ref_eval} -H {path_hyp_eval} -m {metrics} | tee {log_small}\n",
        "else:\n",
        "  path_ref_eval = '/content/GenerationEval/data_to_process/'+lang+'/references/reference'\n",
        "  path_hyp_eval = '/content/GenerationEval/data_to_process/'+lang+'/hypothesis'\n",
        "  for Filepath in glob.glob(os.path.join(path_hyp_uploaded, '*.txt')):\n",
        "    print(f'\\nProcessing {Filepath}...')\n",
        "    log_filename = os.path.join(log_folder, 'log_eval_'+Filepath.rsplit('/',1)[1].split('.')[0]+'.txt')\n",
        "    file_pred_lines = codecs.open(Filepath, 'r', 'utf-8').readlines()\n",
        "    fo = codecs.open(path_hyp_eval, 'w', 'utf-8')\n",
        "    for line in file_pred_lines:\n",
        "      fo.write(line)\n",
        "    fo.close()\n",
        "    ! python {path_code_eval} -R {path_ref_eval} -H {path_hyp_eval} -lng {lang} -nr {num_refs} -m {metrics} | tee {log_filename}\n",
        "\n",
        "# argParser.add_argument(\"-R\", \"--reference\", help=\"reference translation\", required=True)\n",
        "# argParser.add_argument(\"-H\", \"--hypothesis\", help=\"hypothesis translation\", required=True)\n",
        "# argParser.add_argument(\"-lng\", \"--language\", help=\"evaluated language\", default='en')\n",
        "# argParser.add_argument(\"-nr\", \"--num_refs\", help=\"number of references\", type=int, default=4)\n",
        "# argParser.add_argument(\"-m\", \"--metrics\", help=\"evaluation metrics to be computed\", default='bleu,meteor,ter,chrf++,bert,bleurt')\n",
        "# argParser.add_argument(\"-nc\", \"--ncorder\", help=\"chrF metric: character n-gram order (default=6)\", type=int, default=6)\n",
        "# argParser.add_argument(\"-nw\", \"--nworder\", help=\"chrF metric: word n-gram order (default=2)\", type=int, default=2)\n",
        "# argParser.add_argument(\"-b\", \"--beta\", help=\"chrF metric: beta parameter (default=2)\", type=float, default=2.0)\n"
      ],
      "metadata": {
        "id": "iU4X8sPgRexN",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5 - Zip and download log files\n",
        "download_log_files = 'yes'#@param['yes', 'no']\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "if download_log_files == 'yes':\n",
        "  from google.colab import files\n",
        "  zip_name_log = '/content/log_eval.zip'\n",
        "  !zip -r {zip_name_log} {log_folder}\n",
        "\n",
        "  clear_output()\n",
        "\n",
        "  files.download(zip_name_log)"
      ],
      "metadata": {
        "id": "3za-Q6WCTu_B",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}